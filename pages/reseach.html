<!DOCTYPE html>
<html lang="en">

  <head>
      <title>Compumetrika</title>
      <link href='http://fonts.googleapis.com/css?family=Open+Sans:400italic,600italic,700italic,400,600,700' rel='stylesheet' type='text/css' />
      <link href='http://fonts.googleapis.com/css?family=Merriweather:300' rel='stylesheet' type='text/css'/>
      <link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:200,400,700' rel='stylesheet' type='text/css'/>
      <link rel="stylesheet" type="text/css" href="/theme/css/icons.css"/>
      <link rel="stylesheet" type="text/css" href="/theme/css/styles.css"/>
      <meta charset="utf-8" />
  </head>

  <body id="index">
    <!-- header -->
    <header class="siteheader">
      <!-- site image -->

      <div class = "sitebanner">
        <h1><a class="sitetitle nodec" href="">Compumetrika</a></h1>
        <h3 class ="sitesubtitle"></h3>
        <!-- nav -->
        <nav class="menu">
          <ul>
            <!-- menu items-->
            <!--pages-->
                <li><a class="nodec" href="/pages/about.html">About</a></li>
                <li><a class="nodec" href="/pages/contact.html">Contact</a></li>
                <li class="active"><a class="nodec" href="/pages/reseach.html">Research</a></li>
                <li><a class="nodec" href="/pages/resources.html">Resources</a></li>
                <li><a class="nodec" href="/pages/teaching.html">Teaching</a></li>
            <!-- services icons -->
          </ul>
        </nav>
      </div> <!-- sitebanner -->
    </header>

    <!-- content -->
<div class="content">
  <div class="article">
    <h1>Research</h1>
    <h2>Research Summary</h2>
<p>My research deals with dynamic learning when agents do not know the transition probabilities associated with their state space. Agents learn the value of an arbitrary policy function from experience, and improve their policy by comparing their estimate with that of a neighbor. With enough experience, agent policies converge to the optimum. Even aside from the optimum, agents continually seek to improve their policy function and are "boundedly rational" -- doing their best conditional on their current knowledge. Intuitively, agents learn fastest when they can communicate with others and when they "baseline" their actions against hypothetical own experiences.
Intuitively, extensions such as social learning and agent-level introspection (a modification to the value estimator) allow faster learning over the most basic version. </p>
<p>This approach allows agents in complex environments to nonetheless continually learn and improve their policies in a well-understood way. The approach is modular and usable in a wide variety of settings (with one goal of learning analogous to "completely uncoupled" behavior as Nax, Pradelski, and H.P. Young 2013). One intended application is a large-scale agent-based model such as Geanakoplos et al. (2012). Prices are determined by processes in a high-dimensional state-space with non-trivial market structures, both in the general-equilibrium housing market and the partial-equilibrium mortgage and labor markets. Solving for optimal policies in this environment is non-trivial (and perhaps not desirable even if attainable). On the other extreme, agents who do not change their policies as their environment shifts is also not desirable -- even if agents do not unconditionally optimize, we would like them to do the best they can, conditional on their instantaneous state of knowledge. </p>
<h2>Publications &amp; Work in Progress</h2>
<p><a href="../">Home</a></p>
  </div>
</div>

    <!-- footer -->
    <footer>
      <p>
        Â© Nathan Palmer, license <a href=""> </a>
        unless otherwise noted.
        Generated by <a href= "http://docs.getpelican.com/">Pelican</a> with
        <a href="http://github.com/porterjamesj/crowsfoot">crowsfoot</a> theme.
      </p>
    </footer>
  </body>
</html>